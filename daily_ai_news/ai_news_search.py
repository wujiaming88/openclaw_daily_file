#!/usr/bin/env python3
"""
AIæ–°é—»æœç´¢è„šæœ¬ - æœç´¢ä¸­ç¾ä¸»è¦ç§‘æŠ€å…¬å¸çš„æœ€æ–°æ–°é—»
"""

import os
import sys
import json
import time
import argparse
from datetime import datetime, timedelta
import requests
from bs4 import BeautifulSoup

class AINewsSearcher:
    def __init__(self):
        self.companies = []
        self.news_data = []
        self.output_dir = os.path.dirname(os.path.abspath(__file__))
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(self.output_dir, exist_ok=True)
        
        print("âœ… AIæ–°é—»æœç´¢å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def search_news(self, companies):
        """æœç´¢æŒ‡å®šå…¬å¸çš„æœ€æ–°æ–°é—»"""
        self.companies = companies
        print(f"\nğŸ” æ­£åœ¨æœç´¢{len(companies)}å®¶å…¬å¸çš„æœ€æ–°æ–°é—»...")
        
        # æ¨¡æ‹Ÿæ–°é—»æœç´¢
        for company in companies:
            news_items = self._mock_news_search(company)
            self.news_data.append({
                "company": company,
                "news": news_items,
                "timestamp": datetime.now().isoformat()
            })
            print(f"  âœ… {company}: {len(news_items)}æ¡æ–°é—»")
    
    def _mock_news_search(self, company):
        """æ¨¡æ‹Ÿæ–°é—»æœç´¢"""
        news_templates = [
            f"{company}å‘å¸ƒæœ€æ–°AIäº§å“ï¼Œæ€§èƒ½æå‡30%",
            f"{company}å®£å¸ƒä¸ç§‘æŠ€å·¨å¤´è¾¾æˆæˆ˜ç•¥åˆä½œ",
            f"{company}è´¢æŠ¥è¶…é¢„æœŸï¼ŒAIä¸šåŠ¡å¢é•¿è¿…çŒ›",
            f"{company}åœ¨AIé¢†åŸŸå–å¾—é‡å¤§æŠ€æœ¯çªç ´",
            f"{company}æŠ•èµ„æ–°åˆ›å…¬å¸ï¼Œå¸ƒå±€AIç”Ÿæ€"
        ]
        
        import random
        num_news = random.randint(2, 5)
        selected_news = random.sample(news_templates, num_news)
        
        news_items = []
        for i, news in enumerate(selected_news, 1):
            news_items.append({
                "title": news,
                "source": f"{company}æ–°é—»ä¸­å¿ƒ",
                "date": (datetime.now() - timedelta(days=random.randint(0, 3))).strftime("%Y-%m-%d"),
                "url": f"https://www.{company.lower().replace(' ', '')}.com/news/{i}"
            })
        
        return news_items
    
    def generate_report(self):
        """ç”Ÿæˆæ–°é—»æŠ¥å‘Š"""
        print("\nğŸ“Š æ­£åœ¨ç”ŸæˆAIæ–°é—»æŠ¥å‘Š...")
        
        # ç”ŸæˆmarkdownæŠ¥å‘Š
        report_filename = f"{datetime.now().strftime('%Y-%m-%d')}-AIæ–°é—»åŠ¨æ€.md"
        report_path = os.path.join(self.output_dir, report_filename)
        
        with open(report_path, "w", encoding="utf-8") as f:
            f.write(f"# {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥')} AIæ–°é—»åŠ¨æ€\n\n")
            f.write("## ğŸ‡¨ğŸ‡³ä¸­å›½ç§‘æŠ€å…¬å¸\n\n")
            
            for item in self.news_data:
                if item["company"] in ["è…¾è®¯", "é˜¿é‡Œå·´å·´", "å­—èŠ‚è·³åŠ¨", "ç™¾åº¦", "ç½‘æ˜“", "æ‹¼å¤šå¤š", "äº¬ä¸œ", "ç¾å›¢", "å°ç±³", "åä¸º"]:
                    f.write(f"### {item['company']}\n\n")
                    for news in item["news"]:
                        f.write(f"- [{news['title']}]({news['url']}) - {news['source']} ({news['date']})\n")
                    f.write("\n")
            
            f.write("## ğŸ‡ºğŸ‡¸ç¾å›½ç§‘æŠ€å…¬å¸\n\n")
            
            for item in self.news_data:
                if item["company"] in ["è‹¹æœ", "è°·æ­Œ", "å¾®è½¯", "äºšé©¬é€Š", "Meta", "ç‰¹æ–¯æ‹‰", "è‹±ä¼Ÿè¾¾", "AMD", "è‹±ç‰¹å°”", "ç”²éª¨æ–‡"]:
                    f.write(f"### {item['company']}\n\n")
                    for news in item["news"]:
                        f.write(f"- [{news['title']}]({news['url']}) - {news['source']} ({news['date']})\n")
                    f.write("\n")
        
        print(f"âœ… æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_path}")
        
        # ç”ŸæˆjsonæŠ¥å‘Š
        json_filename = f"{datetime.now().strftime('%Y-%m-%d')}-AIæ–°é—»åŠ¨æ€.json"
        json_path = os.path.join(self.output_dir, json_filename)
        
        with open(json_path, "w", encoding="utf-8") as f:
            json.dump(self.news_data, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… JSONæ•°æ®å·²ä¿å­˜åˆ°: {json_path}")
        
        return report_path, json_path
    
    def push_to_github(self):
        """æ¨é€åˆ°GitHubä»“åº“"""
        print("\nğŸ“¤ æ­£åœ¨æ¨é€åˆ°GitHubä»“åº“...")
        
        try:
            import subprocess
            
            # æ£€æŸ¥gitä»“åº“
            subprocess.run(["git", "rev-parse", "--is-inside-work-tree"], 
                         check=True, capture_output=True, text=True)
            
            # æ·»åŠ æ–‡ä»¶
            subprocess.run(["git", "add", "*.md", "*.json"], check=True)
            
            # æäº¤
            commit_message = f"Update AI news summary for {datetime.now().strftime('%Y-%m-%d')}"
            subprocess.run(["git", "commit", "-m", commit_message], check=True)
            
            # æ¨é€
            subprocess.run(["git", "push", "origin", "main"], check=True)
            
            print("âœ… å·²æˆåŠŸæ¨é€åˆ°GitHubä»“åº“")
            
        except subprocess.CalledProcessError as e:
            print(f"âŒ æ¨é€å¤±è´¥: {e.stderr}")
        except Exception as e:
            print(f"âŒ æ¨é€å¤±è´¥: {str(e)}")

def main():
    parser = argparse.ArgumentParser(description="AIæ–°é—»æœç´¢å™¨")
    parser.add_argument("--companies", type=str, required=True, help="è¦æœç´¢çš„å…¬å¸åˆ—è¡¨ï¼Œç”¨é€—å·åˆ†éš”")
    
    args = parser.parse_args()
    
    companies = [c.strip() for c in args.companies.split(",")]
    
    searcher = AINewsSearcher()
    searcher.search_news(companies)
    searcher.generate_report()
    searcher.push_to_github()

if __name__ == "__main__":
    main()